import re
from typing import Any, List, Tuple


def _nemo_tokenizer(num_virtual_tokens: int) -> Tuple:
    """
    Retrieves the NeMo Megatron tokenizer using the NeMo helper modules.

    Args:
        num_virtual_tokens: The number of virtual tokens to add to the prompt for the specified task.

    Returns:
        Returns the NeMo Megatron tokenizer with the pseudo tokens included.
    """
    # Import locally to avoid time penalties for loading the NeMo modules while using the non-NeMo workflow.
    from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer
    from nemo.collections.nlp.models.language_modeling.megatron_gpt_prompt_learning_model import get_pseudo_tokens

    tokenizer = get_nmt_tokenizer('megatron', 'GPT2BPETokenizer')
    pseudo_tokens = get_pseudo_tokens(num_virtual_tokens)
    tokenizer.add_special_tokens({'additional_special_tokens': pseudo_tokens})
    return tokenizer, pseudo_tokens


def _huggingface_pseudo_tokens(num_virtual_tokens: int) -> List:
    """
    Creates a list of pseudo tokens based on the number of virtual prompts as
    determined in the model config file.

    Args:
        num_virtual_tokens: The number of virtual tokens to add to the prompt for the specified task.

    Returns:
        Returns a list of the additional special tokens to add to the tokenizer.
    """
    pseudo_tokens = [
        f'<prompt_{str(num)}>' for num in range(num_virtual_tokens)
    ]
    return pseudo_tokens


def _huggingface_tokenizer(num_virtual_tokens: int) -> Tuple:
    """
    Retrieves the HuggingFace tokenizer as a GPT2Tokenizer for faster processing.

    Args:
        num_virtual_tokens: The number of virtual tokens to add to the prompt for the specified task.

    Returns:
        Returns the GPT2 tokenizer from HuggingFace with the pseudo tokens included.
    """
    # Import locally to avoid time penalties for loading the NeMo modules.
    from transformers import GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    pseudo_tokens = _huggingface_pseudo_tokens(num_virtual_tokens)
    tokenizer.add_special_tokens({'additional_special_tokens': pseudo_tokens})
    return tokenizer, pseudo_tokens


def get_tokenizer(use_nemo: bool, num_virtual_tokens: int) -> Tuple:
    """
    Retrieves either the NeMo or HuggingFace tokenizer as requested by the
    user.

    Args:
        use_nemo: If True, use the NeMo tokenizer. Otherwise use the HuggingFace tokenizer.
        pseudo_tokens: A list of additional special tokens to add to the tokenizer.

    Returns:
        Returns the specified tokenizer with the added special tokens.
    """
    if use_nemo:
        tokenizer, pseudo_tokens = _nemo_tokenizer(num_virtual_tokens)
    else:
        tokenizer, pseudo_tokens = _huggingface_tokenizer(num_virtual_tokens)
    return tokenizer, pseudo_tokens


def get_task_template(config: dict, taskname: str) -> List:
    """
    Convert the task template from the model config to a Python dictionary and
    find the maximum number of virtual tokens the model uses.

    Args:
        config: The config file included with the model.
        taskname: The name of the task to send a request for.

    Returns:
        Returns the converted template as a dictionary as well as the number of virtual tokens to use.
    """
    num_tokens = 0
    task_template = {}

    for task_id_num, task in enumerate(config['task_templates']):
        task_template[task.taskname] = {
            "prompt_template": task.prompt_template,
            "prompt_template_fields": re.findall("\{(.*?)\}", task.prompt_template),
            "answer_only_loss": task.get("answer_only_loss", False),
            "answer_field": task.get("answer_field", None),
            "truncate_field": task.truncate_field,
            "total_virtual_tokens": task.total_virtual_tokens,
            "virtual_token_splits": task.virtual_token_splits,
            "task_id_num": task_id_num
        }
        if task.taskname == taskname:
            num_tokens = task['total_virtual_tokens']
    return task_template, num_tokens


def ids_to_text(tokenizer: Any, ids: List) -> str:
    """
    Convert a list of tokenized IDs to un-tokenized text.

    Args:
        tokenizer: The tokenizer that was used to tokenize the prompt.
        ids: A list of tokenized IDs generated by an inference request.

    Returns:
        Returns a string of the converted text.
    """
    # The NeMo tokenizer has special helper methods to convert IDs to text.
    if hasattr(tokenizer, 'ids_to_text'):
        output = tokenizer.ids_to_text(ids)
    else:
        output_tokens = tokenizer.convert_ids_to_tokens(ids)
        tokens = [token for token in output_tokens \
                  if token not in tokenizer.all_special_tokens]
        output = tokenizer.convert_tokens_to_string(tokens)
    return output
